{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ToM import classify_hypothesis_n_agents, get_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed, compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnt/weka/jrudoler/cis7000-multi-agent/.venv/lib/python3.12/site-packages/dask_jobqueue/core.py:266: FutureWarning: job_extra has been renamed to job_extra_directives. You are still using it (even if only set to []; please also check config files). If you did not set job_extra_directives yet, job_extra will be respected for now, but it will be removed in a future release. If you already set job_extra_directives, job_extra is ignored and you can remove it.\n",
      "  warnings.warn(warn, FutureWarning)\n",
      "/home/mnt/weka/jrudoler/cis7000-multi-agent/.venv/lib/python3.12/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 34357 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from dask_jobqueue import SGECluster\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "\n",
    "n_workers = 100\n",
    "\n",
    "cluster = SGECluster(\n",
    "        cores=1,\n",
    "        memory=\"1GB\",\n",
    "        processes=1,\n",
    "        queue=\"short.q\",\n",
    "        job_extra_directives=[\"-t 1-100\"],\n",
    "        log_directory=os.path.join(os.environ[\"HOME\"], \"logs/\"),\n",
    "        local_directory=os.path.join(os.environ[\"HOME\"], \"dask-worker-space/\"),\n",
    "        walltime=\"01:59:00\",\n",
    "        name=\"cis7000-{$SGE_TASK_ID}\",\n",
    "    )\n",
    "client = Client(cluster)\n",
    "cluster.scale(n=n_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_workers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 65\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Merge with original samples\u001b[39;00m\n\u001b[1;32m     59\u001b[0m final_df \u001b[38;5;241m=\u001b[39m expanded_df\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[1;32m     60\u001b[0m     test_samples, \n\u001b[1;32m     61\u001b[0m     left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     62\u001b[0m     right_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     63\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m dask_df \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mfrom_pandas(final_df, npartitions\u001b[38;5;241m=\u001b[39m\u001b[43mn_workers\u001b[49m)  \u001b[38;5;66;03m# Adjust the number of partitions as needed\u001b[39;00m\n\u001b[1;32m     67\u001b[0m delayed_results \u001b[38;5;241m=\u001b[39m [process_sample(row) \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m dask_df\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[1;32m     68\u001b[0m results \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;241m*\u001b[39mdelayed_results)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_workers' is not defined"
     ]
    }
   ],
   "source": [
    "# set seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# try:\n",
    "#     # Load file if exists\n",
    "#     df_results = pd.read_excel(\"data/results_ToM.csv\")\n",
    "# except:\n",
    "#     df_results = pd.DataFrame(columns=[\"premise\", \"hypothesis\", \"label\", \"predicted\", \"result\"])\n",
    "\n",
    "df_results = pd.DataFrame(columns=[\"premise\", \"hypothesis\", \"label\", \"predicted\", \"result\"])\n",
    "\n",
    "# Config\n",
    "few_shot = [1, 4, 8]  # [1, 4, 8, 16, 32]\n",
    "agents = [1, 3, 5]  # [1, 3, 5, 9]\n",
    "\n",
    "# The batch size of new samples\n",
    "n_test_samples = 10\n",
    "\n",
    "# Add new results\n",
    "# new_samples = ToM.get_n_new_premises(new_n_samples, df_results)\n",
    "\n",
    "@delayed\n",
    "def process_sample(row) -> dict:\n",
    "    sample_premise = row[\"premise\"]\n",
    "    sample_hypothesis = row[\"hypothesis\"]\n",
    "    sample_label = row[\"label\"]\n",
    "    shot = row[\"few_shot\"]\n",
    "    agent = row[\"n_agents\"]\n",
    "    \n",
    "    # Get the prediction with combination of agents and few shot examples\n",
    "    predicted, result = classify_hypothesis_n_agents(sample_hypothesis, sample_premise, sample_label, agent, shot)\n",
    "    new_row = {\n",
    "        \"sample_id\": row[\"sample_id\"],\n",
    "        \"premise\": sample_premise,\n",
    "        \"hypothesis\": sample_hypothesis,\n",
    "        \"n_agents\": agent,\n",
    "        \"m_examples\": shot,\n",
    "        \"label\": sample_label,\n",
    "        \"predicted\": predicted,\n",
    "        \"result\": result,\n",
    "    }\n",
    "    print(f\"Sample ID: {row['sample_id']} - Few shot examples: {shot} - Agents: {agent} - Predicted: {predicted} - Result: {result}\")\n",
    "    return new_row\n",
    "\n",
    "\n",
    "test_data = get_dataset(split=\"test\")\n",
    "test_samples = test_data.sample(n_test_samples)\n",
    "# Create a product of all combinations\n",
    "combinations = pd.MultiIndex.from_product([\n",
    "    test_samples.index,\n",
    "    few_shot,\n",
    "    agents\n",
    "], names=['sample_id', 'few_shot', 'n_agents'])\n",
    "\n",
    "# Create expanded DataFrame with all combinations\n",
    "expanded_df = pd.DataFrame(index=combinations).reset_index()\n",
    "\n",
    "# Merge with original samples\n",
    "final_df = expanded_df.merge(\n",
    "    test_samples, \n",
    "    left_on='sample_id', \n",
    "    right_index=True\n",
    ")\n",
    "\n",
    "dask_df = dd.from_pandas(final_df, npartitions=n_workers)  # Adjust the number of partitions as needed\n",
    "\n",
    "delayed_results = [process_sample(row) for _, row in dask_df.iterrows()]\n",
    "results = compute(*delayed_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Flatten the list of results and create a DataFrame\n",
    "all_results = [item for sublist in results for item in sublist]\n",
    "df_results = pd.concat([df_results, pd.DataFrame(all_results)], ignore_index=True)\n",
    "\n",
    "# Debugging: Check if df_results is empty\n",
    "if df_results.empty:\n",
    "    print(\"Warning: df_results is empty. No data to save.\")\n",
    "else:\n",
    "    # Save results with error handling\n",
    "    try:\n",
    "        df_results.to_csv(\"data/results_ToM.csv\", index=False)\n",
    "        print(\"Results saved!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
